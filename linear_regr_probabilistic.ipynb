{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Interpretation for Linear Regression\n",
    "\n",
    "Why is the least-square cost function $J(\\beta)$ a good choice for regression problems?\n",
    "\n",
    "$$J(\\beta) = \\frac{1}{2}\\sum_{i=1}^n(y_i - \\beta^Tx_i)^2$$\n",
    "\n",
    "Can it be shown to be a natural choice for the regression problem, maybe under certain assumptions? The true relationship between the variables are given by the equation:\n",
    "\n",
    "$$y_i = \\beta^Tx_i + \\epsilon_i$$\n",
    "\n",
    "$\\epsilon_i$ is a catch-all error term for what is missed in the simple model. Let us assume that $\\epsilon$'s are independently and identically distributed (IID) according to a Gaussian distribution, $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$\n",
    "\n",
    "$$p(\\epsilon_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma} exp (-\\frac{\\epsilon_i^2}{2\\sigma^2})$$\n",
    "\n",
    "This implies that the distribution of $y_i$ given $x_i$, parameterized by $\\beta$ is given by\n",
    "\n",
    "$$p(y_i | x_i; \\beta) = \\frac{1}{\\sqrt{2\\pi}\\sigma} exp(-\\frac{(y_i - \\beta^Tx_i)^2}{2\\sigma^2})$$\n",
    "\n",
    "The above probability can be defined as a __likelihood__ function $L(\\beta; X, y)$, and because $\\epsilon$'s are IID:\n",
    "\n",
    "\\begin{align}\n",
    "L(\\beta) & = \\prod_{i = 1}^{n} p(y_i \\ x_i; \\beta) \\\\\n",
    "& = \\prod_{i = 1}^{n} \\frac{1}{\\sqrt{2\\pi}\\sigma} exp(-\\frac{(y_i - \\beta^Tx_i)^2}{2\\sigma^2}) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Given the above relationship between the variables, how do we choose $\\beta$? We actually need to maximize the agreement of the model with the observed data, and in this case, it means we need to select parameters that maximizes the likelihood function. Or the __log likelihood__ $\\ell(\\beta)$, a typical chioce which provides convenience without altering the underlying optimization problem (as it is a strictly increasing function).\n",
    "\n",
    "\\begin{align}\n",
    "\\ell(\\beta) & = log L(\\beta) \\\\\n",
    "& = log \\prod_{i = 1}^{n} \\frac{1}{\\sqrt{2\\pi}\\sigma} exp(-\\frac{(y_i - \\beta^Tx_i)^2}{2\\sigma^2}) \\\\\n",
    "& = \\sum_{i = 1}^{n} log \\frac{1}{\\sqrt{2\\pi}\\sigma} exp(-\\frac{(y_i - \\beta^Tx_i)^2}{2\\sigma^2}) \\\\\n",
    "& = \\sum_{i = 1}^{n} log \\frac{1}{\\sqrt{2\\pi}\\sigma} + \\sum_{i = 1}^{n} log (exp(-\\frac{(y_i - \\beta^Tx_i)^2}{2\\sigma^2})) \\\\\n",
    "& = nlog \\frac{1}{\\sqrt{2\\pi}\\sigma} - \\frac{1}{2\\sigma^2} \\sum_{i = 1}^{n}(y_i - \\beta^Tx_i)^2\n",
    "\\end{align}\n",
    "\n",
    "Maximizing $\\ell(\\beta)$ will work out to the be the same as minimizing out original $J(\\beta)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
