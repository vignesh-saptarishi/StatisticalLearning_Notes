{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Methods for Classification\n",
    "\n",
    "This section deals with Logistic Regression and Linear Discriminant Analysis for classification problems.\n",
    "\n",
    "### Logistic Regression\n",
    "For a binary classification problem with class $0$ or $1$, one could apply regression by least squares. The $X \\hat{\\beta}$ so obtained in this case can be shown to actually be an estimate of $Pr(Y=1 \\mid X)$. But the estimates from linear regression tend to be outside the [0, 1] interval, and hence not useful for a probabilistic interpretation. In Logistic regression, we avoid this problem by applying a function that gives outputs between $0$ to $1$ for all input values - the ___logit___ aka ___sigmoid___ function. \n",
    "\n",
    "Rather than modelling the response variable $Y$ directly, logistic regression models the probability that $Y$ belongs to a particular category, i.e., the conditional probability of $Y$ given the predictors $X$: $p(X) = P(Y=1 \\mid X)$. These take values in the interval $[0, 1]$. We could then predict (or assign) an observation to a particular class based on a threshold probability value. For example, we could assign the observation to class $1$ if $p(X) = Pr(Y = 1 \\mid X) > 0.5$, and to the other class otherwise. Depending on the application, we might also want to be conservative about our prediction and instead choose lower values like $0.1$ as threshold.\n",
    "\n",
    "We model the $p(x)$ as a sigmoid function:\n",
    "\n",
    "$$p(X) = \\frac{\\exp(\\beta_0 + \\beta_1X)}{1 + \\exp(\\beta_0 + \\beta_1X)}$$\n",
    "\n",
    "A bit of algebra would show that:\n",
    "\n",
    "$$\\log \\frac{p(X)}{1 - p(X)} = \\beta_0 + \\beta_1X$$\n",
    "\n",
    "$\\frac{p(X)}{1 - p(X)}$ is called the ___odds___, and the ___log-odds___ (aka logit) is shown to be linear in $X$. For logistic regression, the optimization problem we want to solve is to find estimates for $\\beta$ such that the estimated probability $\\hat p(x_i)$ corresponds as closely as possible to the original training observation. This approach is known as the ___maximum likelihood___ approach. Formally, we seek to find estimates $\\beta$ that maximizes the _likelihood_ function:\n",
    "\n",
    "$$L(\\beta) = \\prod_{i:y_i=1}p(x_i) \\prod_{i^\\prime: y_{i^\\prime} = 0}(1 - p(x_{i^\\prime}))$$\n",
    "\n",
    "or any monotonic function of $L$, specifically the ___log-likelihood___ as is often used for convenience:\n",
    "\n",
    "\\begin{align}\n",
    "\\ell(\\beta)\n",
    "&= \\log L(\\beta)\\\\\n",
    "&= \\sum_{i = 1}^{n} \\{y_i \\log p(x_i) + (1 - y_i)\\log(1 - p(x_i))\\}\\\\\n",
    "&= \\sum_{i = 1}^{n}\\{y_i\\beta^Tx_i - \\log (1 + e^{\\beta^Tx_i})\\}\n",
    "\\end{align}\n",
    "\n",
    "The coefficients estimated from logistic regression refer to the change in log-odds of the response and not the response itself.\n",
    "\n",
    "\n",
    "### Linear Discriminant Analysis\n",
    "In logistic regression, we directly modeled the conditional distribution $Pr(Y = k \\mid X = x)$. In Linear Discriminant Analysis (LDA), we instead model the distribution of predictors $X$ in each class, and then use Bayes's Theorem to compute the estimates for $Pr(Y = k \\mid X = x)$.\n",
    "\n",
    "We have $K \\ge 2$ classes in the response variable. Let $f_k(x)$ be the density function of $X$ in class $k$, and $\\pi_k$ be the prior probability of class $k$,  i.e., the probability that a randomly chosen observation comes from the $k$th class, with $\\sum_{k=1}^K \\pi_k = 1$. Baye's theorem now gives us:\n",
    "\n",
    "$$Pr(Y = k \\mid X = x) = p_k(x) = \\frac{\\pi_kf_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)}$$\n",
    "\n",
    "If we have estimates for $\\pi_k$ and $f_k$, we can compute $p_k(x)$. Lets model $f_k(x)$ as a Gaussian:\n",
    "\n",
    "$$f_k(x) = \\frac{1}{(2 \\pi)^{p / 2} \\lvert\\Sigma\\rvert ^{1/2}} \\exp \\left( \\frac{1}{2}(x - \\mu)^T\\Sigma^{-1}(x - \\mu) \\right)$$\n",
    "\n",
    "where $p$ is the number of predictors, $\\mu$ is the mean of $X$ and $\\Sigma = Cov(X)$ with $\\Sigma_k = \\Sigma$ $\\forall k$. In practice, we do not know the parameters of the Gaussian distributions, and hence have to be estimated:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat \\pi_k &= \\frac{n_k}{n} \\\\\n",
    "\\hat \\mu_k &= \\frac{1}{n_k} \\sum_{i:y_i=k}x_i \\\\\n",
    "\\hat \\Sigma &= \\frac{1}{n - K} \\sum_{k=1}^K \\sum_{i:y_i=k} (x_i - \\hat \\mu_k) (x_i - \\hat \\mu_k)^T\n",
    "\\end{align}\n",
    "\n",
    "where $n_k$ is the number of class-$k$ observations. Once we have all this, we can calculate $p_k(x)$ and assign observation $X = x$ to the class for which $p_k(x)$ is largest. Some algebra will show that, this problem is equivalent to assigning the observation to the class for which the ___linear discriminant function___, $\\delta_k$ is largest:\n",
    "\n",
    "$$\\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k + \\log \\pi_k $$\n",
    "\n",
    "We can see that the discriminant function (and also log-odds) is linear in $X$. LDA makes a lot of assumptions about the distribution of the observations, and hence must be considered carefully before using. If the equality of covariances assumption is not met, one can consider using ___Quadratic Discriminant Analysis___ (QDA) which produces quadratic decision boundaries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
