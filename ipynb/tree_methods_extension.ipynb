{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending Tree-based Methods: Bagging, Random Forest, Boosting\n",
    "\n",
    "The basic idea behind all these methods is to use decision trees as a building block to construct more powerful models.\n",
    "\n",
    "\n",
    "### Bagging\n",
    "Trees are ideal for bagging as they can capture complex interactions, and can be grown deep to reduce bias. We generate $B$ bootstrapped samples from the training data, and fit $B$ deep, un-pruned trees $\\hat f^1(x), ..., \\hat f^B(x)$ to each sample. We can then average the $B$ trees to obtain:\n",
    "\n",
    "$$\\hat f_{bag}(x) = \\frac1B \\sum_{b=1}^B \\hat f^b(x)$$\n",
    "\n",
    "This reduces the variance and improves prediction, however at the expense of interpretability. Also, there is a high chance for correlation between the bagged trees. If there is one strong predictor in the dataset, then each tree in the collection of bagged trees will use the same strong predictor for the first split, and often leads to scenarios where the bagged trees are very similar and correlated. In such cases, bagging will not lead to a substantial decrease in variance.\n",
    "\n",
    "\n",
    "### Random Forest\n",
    "Random Forest provides an improvement over Bagging trees in terms of reducing variance, by decorrelating the trees. This is achieved in the tree building process: before each split, $m \\le p$ predictors are chosen at random as candidates for splitting, instead of the entire set of predictors, as in decision trees. When lesser number of predictors are chosen at random, strong predictors are not considered at every split, and hence other predictors can be given a change. This results in very reduced variance, and hence more reliable. Typically, $m = \\sqrt p$. One can see that when $m = p$, random forest is exactly bagging.\n",
    "\n",
    "\n",
    "### Boosting Trees\n",
    "Boosting is a method in which several weak learners are combined to produce one strong learner. A weak learner is one whose error-rate is only slightly better than random guessing. The idea is to sequentially apply a weak learning algorithm (slow learning) to repeatedly modified versions of the data and produce a sequence of weak learners $\\hat f_m(x)$ for $m = 1, 2, ..., M$. The data is modified such that observations that were misclassified at the previous steps get increasing influence in the future steps. This way successive learners are forced to concentrate on observations missed by previous learners. The predictions can then be averaged or put through a majority vote based on weights of the separate learners, to produce the final prediction.\n",
    "\n",
    "Let's consider the AdaBoost algorithm for classification. We start with observation weights $w_i = \\frac1N$ for all observations $i = 1, ..., n$. We then perform slow-learning iteratively. For $m = 1$ to $M$:\n",
    "\n",
    "- Fit classifier $\\hat f_m(x)$ to the training data using weights $w_i$ (weighted observations).\n",
    "- Compute weighted error-rate. For classification:\n",
    "\n",
    "$$err_m = \\frac{\\sum_{i=1}^n w_i I(y_i \\ne \\hat f_m(x_i))} {\\sum_{i=1}^n w_i}$$\n",
    "\n",
    "- Compute weight $\\alpha_m$ given to classifier $\\hat f_m$ in producing the final classifier:\n",
    "\n",
    "$$\\alpha_m = \\log \\left( \\frac{1 - err_m}{err_m} \\right)$$\n",
    "\n",
    "- Update individual weights for next iteration such that misclassified observations get their weights scaled:\n",
    "\n",
    "$$w_i \\leftarrow w_i \\exp[\\alpha_m I(y_i \\ne \\hat f_m(x_i))]$$\n",
    "\n",
    "The final classifier is given by:\n",
    "\n",
    "$$\\hat f(x) = sign [\\sum_{m-1}^M \\alpha_m \\hat f_m(x)]$$\n",
    "\n",
    "For Boosting trees, this basically amounts to starting with $\\hat f(x) = 0$, residuals $r_i = y_i$, fitting small trees, typically stumps with a single split, $\\hat f_m(x)$ to the residuals, and updating $\\hat f(x)$ by adding a shrunken version of the tree:\n",
    "\n",
    "$$\\hat f(x) \\leftarrow \\hat f(x) + \\lambda \\hat f_m(x)$$\n",
    "\n",
    "With residuals updated to:\n",
    "\n",
    "$$r_i \\leftarrow r_i - \\lambda \\hat f_m(x_i)$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
