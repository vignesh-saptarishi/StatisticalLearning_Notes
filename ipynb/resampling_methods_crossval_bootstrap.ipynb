{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling methods: Cross-Validation and Bootstrap\n",
    "\n",
    "The choice of a statistical learning method is guaranteed if it results in low _test-error rate_. The error estimated by using the same points that were used to train the estimator is the _train-error rate_. However, the train-error rate is not a good measure as it can completely under-estimate the test-error rate. Train-error rate can be calculated easily if we have a separate test dataset, but if for any reason we do not have a test dataset, there are several approaches one can use to estimate test-error rate. There are methods that make mathematical adjustments to the train-error rate, such as $AIC$, $BIC$ and adjusted $R^2$. Here we consider Cross-Validation and Bootstrapping.\n",
    "\n",
    "\n",
    "### Cross-Validation\n",
    "#### Validation-Set\n",
    "We randomly divide the data into two datasets - we use one for training and the other to test the estimator, and directly obtain a test-error rate. However, this results in high variance in the test-error rate. If we repeat the process of randomly dividing the dataset into test and train sets, our estimates for test-error rate will exhibit high variance.\n",
    "\n",
    "#### Leave-One-Out Cross-Validation (LOOCV)\n",
    "In LOOCV, we again divide the dataset into two sets - train and test, but we choose only one observation for the test set. We train the model on $n-1$ observations and use the hold-out observation, say $(x_1, y_1)$ and get an estimate for test-error rate $MSE_1$. We can repeat the process $n$ times, leaving out one of the observations each time and get $n$ estimates $MSE_1, ..., MSE_n$. LOOCV estimate for the test-error rate is the average of these values.\n",
    "\n",
    "$$CV = \\frac1n \\sum_{i=1}^n MSE_i$$\n",
    "\n",
    "For classification problems, we can use the number of misclassified observations as a measure of error.\n",
    "\n",
    "$$CV = \\frac1n \\sum_{i=1}^n Err_i$$\n",
    "\n",
    "where $Err_i = I(y_i \\ne \\hat y_i)$, and $I$ is the indicator function.\n",
    "\n",
    "In LOOCV, we repeatedly fit the data using $n-1$ observations (which end up being mostly similar except for 1 observation), which results in very low bias and hence better error estimates. However, we average the effect of $n$ models with $n-1$ almost similar observations, and this results in high correlation, and hence high variance. \n",
    "\n",
    "\n",
    "#### k-Fold Cross-Validation\n",
    "Here, we divide the observations into $k$ groups. We use $k-1$ groups to train the model, and $1$ group to get an error estimate. This repeated $k$ times results in $k$ estimates $MSE_1, ..., MSE_k$. The estimate is simply the average of these values.\n",
    "\n",
    "$$CV = \\frac1k \\sum_{i=1}^k MSE_i$$\n",
    "\n",
    "LOOCV can be seen to be a special-case of k-Fold CV where $k = n$. In k-Fold, typically $k < n$, and is therefore computationally less expensive. This also results in lower variance, because the output of lesser number of models with less similar observations are not correlated as much, compared to LOOCV.\n",
    "\n",
    "\n",
    "### Bootstrap Methods\n",
    "Suppose we have a set of observations that we can use to train a model, $Z = (z_1, z_2 ... z_n)$, where $z_i = (x_i, y_y)$. We can then randomly draw observations from $Z$ with replacement, till we have a dataset of size $n$, say $Z_1$. We can repeat this process $B$ times to get $B$ bootstrapped datasets $Z^{*1}, Z^{*2} ... Z^{*B}$. For a large $n$, the set $Z^{*i}$ is expected to have $1 - \\frac1e = 0.632$ fraction of unique values from $Z$, the rest being duplicates. We can now use $B$ datasets to fit $B$ different models, and look at the distribution of our variable of interest say $S(Z)$. We can obtain quantities like Standard errors as:\n",
    "\n",
    "$$\\widehat {\\mathbf{SE}}[S(Z)] = \\sqrt{ \\frac1{B-1} \\sum_{b=1}^B(S(Z^{*b})) - \\bar S)^2 }$$\n",
    "\n",
    "where $\\bar S = \\frac1B\\sum_b S(Z^{*b})$. We could also calculate the test-error using bootstrap. Let $\\hat y^{*b}(x_i)$ be the predicted value at $x_i$ from the model fitted to the $b$th bootstrap $Z^{*b}$. The test-error rate is:\n",
    "\n",
    "$$\\widehat{Err}_{boot} = \\frac1B \\frac1n \\sum_{b=1}^B \\sum_{i=1}^n L(y_i, \\hat y^{*b}(x_i))$$\n",
    "\n",
    "We again face problems of overlap in dataset when we test $B$ models on the same original dataset. We could imagine doing leave-one-out bootstrap where we keep track of predictions from bootstrap samples that does not contain the observation.\n",
    "\n",
    "#### Bootstrap Aggregation (Bagging)\n",
    "Bootstrap Aggregation, aka Bagging, is an ensemble algorithm where $B$ separate models are fit to $B$ bootstrap samples and the predictions are averaged. We calculate $\\hat f^1(x), \\hat f^2(x) ..., \\hat f^b(x)$ for the $B$ bootstrapped training sets and obtain one low-variance statistical model given by:\n",
    "\n",
    "$$\\hat f_{bag} = \\frac1B \\sum_{b=1}^B \\hat f^b(x)$$\n",
    "\n",
    "Bagging can be used for any method, but it is particularly used for decision trees.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
